%PDF-1.4
%“Œ‹ž ReportLab Generated PDF document http://www.reportlab.com
1 0 obj
<<
/F1 2 0 R /F2 3 0 R /F3 13 0 R /F4 17 0 R /F5 21 0 R
>>
endobj
2 0 obj
<<
/BaseFont /Helvetica /Encoding /WinAnsiEncoding /Name /F1 /Subtype /Type1 /Type /Font
>>
endobj
3 0 obj
<<
/BaseFont /Helvetica-Bold /Encoding /WinAnsiEncoding /Name /F2 /Subtype /Type1 /Type /Font
>>
endobj
4 0 obj
<<
/A <<
/S /URI /Type /Action /URI (mailto:mkhoshle@asu.edu)
>> /Border [ 0 0 0 ] /Rect [ 153.7323 738.7736 242.2523 750.7736 ] /Subtype /Link /Type /Annot
>>
endobj
5 0 obj
<<
/A <<
/S /URI /Type /Action /URI (mailto:obeckste@asu.edu)
>> /Border [ 0 0 0 ] /Rect [ 153.7323 693.7736 239.4823 705.7736 ] /Subtype /Link /Type /Annot
>>
endobj
6 0 obj
<<
/A <<
/S /URI /Type /Action /URI (http://dask.pydata.org)
>> /Border [ 0 0 0 ] /Rect [ 205.2495 522.0236 228.0295 534.0236 ] /Subtype /Link /Type /Annot
>>
endobj
7 0 obj
<<
/A <<
/S /URI /Type /Action /URI (http://mdanalysis.org)
>> /Border [ 0 0 0 ] /Rect [ 102.7041 510.0236 155.4841 522.0236 ] /Subtype /Link /Type /Annot
>>
endobj
8 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 24 0 R /XYZ 68.69291 441.9843 0 ] /Rect [ 484.4507 510.0236 529.8027 522.0236 ] /Subtype /Link /Type /Annot
>>
endobj
9 0 obj
<<
/Annots [ 4 0 R 5 0 R 6 0 R 7 0 R 8 0 R ] /Contents 43 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 42 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 
  /Trans <<

>> /Type /Page
>>
endobj
10 0 obj
<<
/A <<
/S /URI /Type /Action /URI (http://mdanalysis.org)
>> /Border [ 0 0 0 ] /Rect [ 62.69291 729.0236 115.4729 741.0236 ] /Subtype /Link /Type /Annot
>>
endobj
11 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 24 0 R /XYZ 68.69291 441.9843 0 ] /Rect [ 109.1192 705.0236 154.4712 717.0236 ] /Subtype /Link /Type /Annot
>>
endobj
12 0 obj
<<
/A <<
/S /URI /Type /Action /URI (http://dask.pydata.org)
>> /Border [ 0 0 0 ] /Rect [ 471.5658 633.0236 494.3458 645.0236 ] /Subtype /Link /Type /Annot
>>
endobj
13 0 obj
<<
/BaseFont /Symbol /Encoding /SymbolEncoding /Name /F3 /Subtype /Type1 /Type /Font
>>
endobj
14 0 obj
<<
/Annots [ 10 0 R 11 0 R 12 0 R ] /Contents 44 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 42 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 
  /Trans <<

>> /Type /Page
>>
endobj
15 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 24 0 R /XYZ 68.69291 441.9843 0 ] /Rect [ 204.1975 705.0236 249.5495 717.0236 ] /Subtype /Link /Type /Annot
>>
endobj
16 0 obj
<<
/Annots [ 15 0 R ] /Contents 45 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 42 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 
  /Trans <<

>> /Type /Page
>>
endobj
17 0 obj
<<
/BaseFont /Helvetica-BoldOblique /Encoding /WinAnsiEncoding /Name /F4 /Subtype /Type1 /Type /Font
>>
endobj
18 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 9 0 R /XYZ 470.0129 524.0236 0 ] /Rect [ 128.7129 427.9843 134.2729 439.9843 ] /Subtype /Link /Type /Annot
>>
endobj
19 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 14 0 R /XYZ 108.2729 719.0236 0 ] /Rect [ 68.69291 415.9843 74.25291 427.9843 ] /Subtype /Link /Type /Annot
>>
endobj
20 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 16 0 R /XYZ 200.5129 719.0236 0 ] /Rect [ 79.81291 415.9843 85.37291 427.9843 ] /Subtype /Link /Type /Annot
>>
endobj
21 0 obj
<<
/BaseFont /ZapfDingbats /Encoding /ZapfDingbatsEncoding /Name /F5 /Subtype /Type1 /Type /Font
>>
endobj
22 0 obj
<<
/A <<
/S /URI /Type /Action /URI (http://mdanalysis.org)
>> /Border [ 0 0 0 ] /Rect [ 281.4323 349.9843 373.6923 361.9843 ] /Subtype /Link /Type /Annot
>>
endobj
23 0 obj
<<
/A <<
/S /URI /Type /Action /URI (https://doi.org/10.6084/m9.figshare.4695742)
>> /Border [ 0 0 0 ] /Rect [ 169.8523 319.9843 302.7123 331.9843 ] /Subtype /Link /Type /Annot
>>
endobj
24 0 obj
<<
/Annots [ 18 0 R 19 0 R 20 0 R 22 0 R 23 0 R ] /Contents 46 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 42 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 
  /Trans <<

>> /Type /Page
>>
endobj
25 0 obj
<<
/Outlines 27 0 R /PageLabels 47 0 R /PageMode /UseNone /Pages 42 0 R /Type /Catalog
>>
endobj
26 0 obj
<<
/Author (Oliver Beckstein) /CreationDate (D:20170529154835+07'00') /Creator (\(unspecified\)) /Keywords () /ModDate (D:20170529154835+07'00') /Producer (ReportLab PDF Library - www.reportlab.com) 
  /Subject (\(unspecified\)) /Title () /Trapped /False
>>
endobj
27 0 obj
<<
/Count 16 /First 28 0 R /Last 28 0 R /Type /Outlines
>>
endobj
28 0 obj
<<
/Count 13 /Dest [ 9 0 R /XYZ 62.69291 636.0236 0 ] /First 29 0 R /Last 41 0 R /Parent 27 0 R /Title (Parallel Analysis in MDAnalysis using the Dask Parallel Computing Library)
>>
endobj
29 0 obj
<<
/Dest [ 9 0 R /XYZ 62.69291 582.0236 0 ] /Next 30 0 R /Parent 28 0 R /Title (Abstract)
>>
endobj
30 0 obj
<<
/Dest [ 9 0 R /XYZ 62.69291 354.0236 0 ] /Next 31 0 R /Parent 28 0 R /Prev 29 0 R /Title (Keywords)
>>
endobj
31 0 obj
<<
/Dest [ 14 0 R /XYZ 62.69291 765.0236 0 ] /Next 32 0 R /Parent 28 0 R /Prev 30 0 R /Title (Introduction)
>>
endobj
32 0 obj
<<
/Dest [ 16 0 R /XYZ 62.69291 765.0236 0 ] /Next 33 0 R /Parent 28 0 R /Prev 31 0 R /Title (Effect of I/O Environment)
>>
endobj
33 0 obj
<<
/Dest [ 16 0 R /XYZ 62.69291 381.0236 0 ] /Next 34 0 R /Parent 28 0 R /Prev 32 0 R /Title (Effect of File Format)
>>
endobj
34 0 obj
<<
/Dest [ 24 0 R /XYZ 62.69291 765.0236 0 ] /Next 35 0 R /Parent 28 0 R /Prev 33 0 R /Title (Challenges for Good HPC Performance)
>>
endobj
35 0 obj
<<
/Count 4 /Dest [ 24 0 R /XYZ 62.69291 735.0236 0 ] /First 36 0 R /Last 39 0 R /Next 40 0 R /Parent 28 0 R 
  /Prev 34 0 R /Title (Performance Optimization)
>>
endobj
36 0 obj
<<
/Dest [ 24 0 R /XYZ 62.69291 705.0236 0 ] /Next 37 0 R /Parent 35 0 R /Title (Effect of Striping)
>>
endobj
37 0 obj
<<
/Dest [ 24 0 R /XYZ 62.69291 678.0236 0 ] /Next 38 0 R /Parent 35 0 R /Prev 36 0 R /Title (Effect of Oversubscribing)
>>
endobj
38 0 obj
<<
/Dest [ 24 0 R /XYZ 62.69291 651.0236 0 ] /Next 39 0 R /Parent 35 0 R /Prev 37 0 R /Title (Examining Scheduler Overhead)
>>
endobj
39 0 obj
<<
/Dest [ 24 0 R /XYZ 62.69291 624.0236 0 ] /Parent 35 0 R /Prev 38 0 R /Title (Scheduler Plugin Results)
>>
endobj
40 0 obj
<<
/Dest [ 24 0 R /XYZ 62.69291 597.0236 0 ] /Next 41 0 R /Parent 28 0 R /Prev 35 0 R /Title (Comparison of Performance of Map-Reduce Job Between MPI for Python and Dask Frameworks)
>>
endobj
41 0 obj
<<
/Dest [ 24 0 R /XYZ 62.69291 549.0236 0 ] /Parent 28 0 R /Prev 40 0 R /Title (References)
>>
endobj
42 0 obj
<<
/Count 4 /Kids [ 9 0 R 14 0 R 16 0 R 24 0 R ] /Type /Pages
>>
endobj
43 0 obj
<<
/Length 4809
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 750.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 36.93937 0 Td (Author:) Tj T* -36.93937 0 Td ET
Q
Q
q
1 0 0 1 91.03937 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Mahzad Khoshlessan) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 735.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 44.13937 0 Td (email:) Tj T* -44.13937 0 Td ET
Q
Q
q
1 0 0 1 91.03937 3 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (mkhoshle@asu.edu) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 720.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 21.37937 0 Td (institution:) Tj T* -21.37937 0 Td ET
Q
Q
q
1 0 0 1 91.03937 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Arizona State University) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 705.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 36.93937 0 Td (Author:) Tj T* -36.93937 0 Td ET
Q
Q
q
1 0 0 1 91.03937 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Oliver Beckstein) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 690.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 44.13937 0 Td (email:) Tj T* -44.13937 0 Td ET
Q
Q
q
1 0 0 1 91.03937 3 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (obeckste@asu.edu) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 675.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 21.37937 0 Td (institution:) Tj T* -21.37937 0 Td ET
Q
Q
q
1 0 0 1 91.03937 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Arizona State University) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 648.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F2 10 Tf 12 TL 3.02937 0 Td (corresponding) Tj T* 66.68 0 Td (:) Tj T* -69.70937 0 Td ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 594.0236 cm
q
BT 1 0 0 1 0 24.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Parallel Analysis in MDAnalysis using the Dask Parallel) Tj T* (Computing Library) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 564.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Abstract) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 366.0236 cm
q
BT 1 0 0 1 0 182 Tm -0.023588 Tw 12 TL /F1 10 Tf 0 0 0 rg (The analysis of biomolecular computer simulations has become a challenge because the amount of output) Tj T* 0 Tw 2.358876 Tw (data is now routinely in the terabyte range. We evaluate if this challenge can be met by a parallel) Tj T* 0 Tw 1.179147 Tw (map-reduce approach with the ) Tj 0 0 .501961 rg (Dask) Tj 0 0 0 rg ( parallel computing library for task-graph based computing coupled) Tj T* 0 Tw 1.110597 Tw (with our ) Tj 0 0 .501961 rg (MDAnalysis) Tj 0 0 0 rg ( Python library for the analysis of molecular dynamics \(MD\) simulations ) Tj /F1 8 Tf 0 0 .501961 rg 5 Ts (Gowers2016) Tj /F1 10 Tf 0 0 0 rg 0 Ts (.) Tj T* 0 Tw 1.889069 Tw (We performed a representative performance evaluation, taking into account the highly heterogeneous) Tj T* 0 Tw .178555 Tw (computing environment that researchers typically work in together with the diversity of existing file formats) Tj T* 0 Tw 1.19686 Tw (for MD trajectory data. We found that the the underlying storage system \(solid state drives, parallel file) Tj T* 0 Tw 2.683984 Tw (systems, or simple spinning platter disks\) can be a deciding performance factor that leads to data) Tj T* 0 Tw .439398 Tw (ingestion becoming the primary bottle neck in the analysis work flow. However, the choice of the data file) Tj T* 0 Tw 1.322651 Tw (format can mitigate the effect of the storage system; in particular, the commonly used "Gromacs XTC") Tj T* 0 Tw 1.19686 Tw (trajectory format, which is highly compressed, can exhibit strong scaling close to ideal due to trading a) Tj T* 0 Tw -0.109249 Tw (decrease in global storage access load against an increase in local per-core cpu-intensive decompression.) Tj T* 0 Tw .236651 Tw (Scaling was tested on single node and multiple nodes on national and local supercomputing resources as) Tj T* 0 Tw 1.224104 Tw (well as typical workstations. In summary, we show that, due to the focus on high interoperability in the) Tj T* 0 Tw .49784 Tw (scientific Python eco system, it is straightforward to implement map-reduce with Dask in MDAnalysis and) Tj T* 0 Tw (provide an in-depth analysis of the considerations to obtain good parallel performance on HPC resources.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 336.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Keywords) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 318.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (MDAnalysis, High Performance Computing, Dask, Map-Reduce, MPI) Tj T* ET
Q
Q
 
endstream
endobj
44 0 obj
<<
/Length 6022
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 747.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Introduction) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 201.0236 cm
q
BT 1 0 0 1 0 530 Tm -0.051265 Tw 12 TL /F1 10 Tf 0 0 .501961 rg (MDAnalysis) Tj 0 0 0 rg ( is a Python library that provides users with access to raw simulation data that allows structural) Tj T* 0 Tw 1.990751 Tw (and temporal analysis of molecular dynamics \(MD\) trajectories generated by all major MD simulation) Tj T* 0 Tw .846251 Tw (packages ) Tj /F1 8 Tf 0 0 .501961 rg 5 Ts (Gowers2016) Tj /F1 10 Tf 0 0 0 rg 0 Ts (. The size of these trajectories is growing as the simulation times is being extended) Tj T* 0 Tw .45152 Tw (from micro-seconds to milli-seconds and larger systems with increasing numbers of atoms are simulated.) Tj T* 0 Tw 2.10528 Tw (Thus the amount of data to be analyzed is growing rapidly \(into the terabyte range\) and analysis is) Tj T* 0 Tw 1.473059 Tw (increasingly becoming a bottleneck. Therefore, there is a need for high performance computing \(HPC\)) Tj T* 0 Tw .492126 Tw (approaches to increase the throughput. MDAnalysis does not yet provide a standard interface for parallel) Tj T* 0 Tw 1.896342 Tw (analysis; instead, various existing parallel libraries are currently used to parallelize MDAnalysis-based) Tj T* 0 Tw 3.226905 Tw (code. Here we evaluate performance for parallel map-reduce type analysis with the ) Tj 0 0 .501961 rg (Dask) Tj 0 0 0 rg ( parallel) Tj T* 0 Tw .618443 Tw (computing library for task-graph based distributed computing on HPC and local computing resources. As) Tj T* 0 Tw 1.99436 Tw (the computational task we perform an optimal structural superposition of the atoms of a protein to a) Tj T* 0 Tw 2.19436 Tw (reference structure by minimizing the RMSD of the C) Tj /F3 10 Tf 12 TL (a) Tj /F1 10 Tf 12 TL (. A range of commonly used MD file formats) Tj T* 0 Tw .941797 Tw (\(CHARMM/NAMD DCD, Gromacs XTC, Amber NetCDF\) and different trajectory sizes are benchmarked) Tj T* 0 Tw 2.399069 Tw (on different HPC resources including national supercomputers \(XSEDE TACC Stampede and SDSC) Tj T* 0 Tw 2.029976 Tw (Comet\), university supercomputers \(ASU Research computing center \(Saguaro\)\), and local resources) Tj T* 0 Tw 1.305976 Tw (\(Gigabit networked multi-core workstations\). All resources architectures are parallel and heterogeneous) Tj T* 0 Tw .83229 Tw (with different CPUs, file systems, high speed networks and are suitable for high-performance distributed) Tj T* 0 Tw 1.084147 Tw (computing at various levels of parallelization. Such a heterogeneous environment creates a challenging) Tj T* 0 Tw 4.328314 Tw (problem for developing high performance programs without the effort required to use low-level,) Tj T* 0 Tw 2.752976 Tw (architecture specific parallel programming models for our domain-specific problem. Different storage) Tj T* 0 Tw 1.096235 Tw (systems such as solid state drives \(SSDs\), hard disk drives \(HDDs\), and the parallel Lustre file system) Tj T* 0 Tw 3.31436 Tw (\(implemented on top of HDD\) are also tested to examine effect of I/O on the performance. The) Tj T* 0 Tw 1.20784 Tw (benchmarks are performed both on a single node and across multiple nodes using the multiprocessing) Tj T* 0 Tw .029431 Tw (and distributed schedulers in Dask library. A protein system of N = 3341 atoms per frame but with different) Tj T* 0 Tw .375697 Tw (number of frames per trajectory which corresponds to different trajectory sizes of \(50GB, 150GB, 300GB\)) Tj T* 0 Tw 2.321412 Tw (for Dask multiprocessing and \(100GB, 300GB, 600GB\) for Dask distributed. All the results for Dask) Tj T* 0 Tw 1.684983 Tw (distributed are obtained across three nodes on different clusters. Results are compared across all file) Tj T* 0 Tw 2.07152 Tw (formats, trajectory sizes, and machines. Our results show strong dependency on the storage system) Tj T* 0 Tw .67686 Tw (because a key problem is competition for access to the same file from multiple processes. However, the) Tj T* 0 Tw 1.08811 Tw (exact data access pattern depends on the trajectory file format and a strong dependence on the actual) Tj T* 0 Tw .17784 Tw (data format arises. Some trajectory formats are more robust against storage system specifics than others.) Tj T* 0 Tw 1.241318 Tw (In particular, analysis with the Gromacs XTC format can show strong ideal scaling over multiple nodes) Tj T* 0 Tw .05784 Tw (because this highly compressed format effectively reduces \(global\) I/O at the expense of increasing \(local\)) Tj T* 0 Tw .432485 Tw (per-core work for decompression. Our results show that there can be other challenges aside from the I/O) Tj T* 0 Tw -0.063588 Tw (bottleneck for achieving good speed-up. For instance, with numbers of processes matched to the available) Tj T* 0 Tw -0.052955 Tw (cores, contention on the network may slow down individual tasks and lead to poor load balancing and poor) Tj T* 0 Tw .969983 Tw (overall performance. In order to identify the performance bottlenecks for our Map-Reduce Job, we have) Tj T* 0 Tw 2.952706 Tw (tested and examined several other factors including striping, oversubscribing, Dask Scheduler, and) Tj T* 0 Tw .445542 Tw (Scheduler Plugin. In addition, lower level tools like MPI for python is used to be compared with high level) Tj T* 0 Tw .244692 Tw (Dask parall library. This will be specially helpful to identify the possible underlying factors that may lead to) Tj T* 0 Tw 2.10998 Tw (low performance. In summary, Dask together with MDAnalysis makes it straightforward to implement) Tj T* 0 Tw 1.249983 Tw (parallel analysis of MD trajectories within a map-reduce scheme. We show that obtaining good parallel) Tj T* 0 Tw 1.129269 Tw (performance depends on multiple factors such as storage system and trajectory file format and provide) Tj T* 0 Tw 1.207126 Tw (guide lines for how to optimize trajectory analysis throughput within the constraints of a heterogeneous) Tj T* 0 Tw (research computing environment.) Tj T* ET
Q
Q
 
endstream
endobj
45 0 obj
<<
/Length 5885
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 747.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Effect of I/O Environment) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 393.0236 cm
q
BT 1 0 0 1 0 338 Tm .574985 Tw 12 TL /F1 10 Tf 0 0 0 rg (In MDAnalysis library, trajectories from MD simulations are a frame by frame description of the motion of) Tj T* 0 Tw .637045 Tw (particles as a function of time. To allow the analysis of large trajectories, MDAnalysis only loads a single) Tj T* 0 Tw .614098 Tw (frame into memory at any time ) Tj /F1 8 Tf 0 0 .501961 rg 5 Ts (Gowers2016) Tj /F1 10 Tf 0 0 0 rg 0 Ts (. Some file systems are designed to run on a single CPU while) Tj T* 0 Tw .05311 Tw (others like Network File System \(NFS\) which is among distributed file systems are designed to let different) Tj T* 0 Tw .217126 Tw (processes on multiple computers access a common set of files. These file systems guarantees sequential) Tj T* 0 Tw .154985 Tw (consistency which means that it prevents any process from reading a file while another process is reading) Tj T* 0 Tw 3.076412 Tw (the file. Distributed parallel file systems \(Lustre\) allow simultaneous access to the file by different) Tj T* 0 Tw .148221 Tw (processes; however it is very important to have a parallel I/O library; otherwise the file system will process) Tj T* 0 Tw -0.015276 Tw (the I/O requests it gets serially, yielding no real benefit from doing parallel I/O. For XTC file format, file size) Tj T* 0 Tw 1.726412 Tw (is smaller as compared to the other formats because of in-built compression. In addition, MDAnalysis) Tj T* 0 Tw 1.727126 Tw (implements a fast frame scanning algorithm for XTC files. This algorithm computes frame offsets and) Tj T* 0 Tw 1.206988 Tw (saves the offsets to disk as a hidden file once the trajectory is read the first time. When a trajectory is) Tj T* 0 Tw .103876 Tw (loaded again then instead of reading the whole trajectory the offset is used to seek individual frames. As a) Tj T* 0 Tw 1.256179 Tw (result, the time it takes a process to load a frame into memory is short. In fact, each frame I/O will be) Tj T* 0 Tw .217356 Tw (followed by decompressing of that frame as soon as it is loaded into memory. Thus, as soon as the frame) Tj T* 0 Tw .48104 Tw (is loaded into memory by one process, the file system will let the next process to load another frame into) Tj T* 0 Tw .60686 Tw (memory. This happens while the first process is decompressing the loaded frame. Figure [] show the I/O) Tj T* 0 Tw 1.609318 Tw (pattern compared between different file formats. For XTC file format, sort of pipelining is happening []) Tj T* 0 Tw 2.16811 Tw (which means that as soon as one process finishes frame I/O, and start decompressing it, the other) Tj T* 0 Tw 1.434987 Tw (process can start another frame I/O. However, this is not the case for DCD and netCDF file format [].) Tj T* 0 Tw -0.092392 Tw (There is no in- built compression for these types of file formats and as a result file sizes are larger. This will) Tj T* 0 Tw 1.878221 Tw (result in higher I/O time \(Which is the bottleneck here\) and therefore, the whole time one process is) Tj T* 0 Tw .417209 Tw (loading a frame into memory other processes are waiting. The I/O time is larger for netCDF file format as) Tj T* 0 Tw 2.067485 Tw (compared to DCD file format. This is since netCDF has a more complicated file format. Reading an) Tj T* 0 Tw 3.759069 Tw (existing netCDF dataset involves opening the dataset; inquiring about dimensions, variables, and) Tj T* 0 Tw 1.236651 Tw (attributes; reading variable data; and closing the dataset [ref]. In fact, netCDF has a very sophisticated) Tj T* 0 Tw .134198 Tw (format, while DCD has a very simple file format. This is why DCD is showing a weak scaling by increasing) Tj T* 0 Tw .964597 Tw (parallelism whereas netCDF file format is being scaled reasonably well by increasing parallelism across) Tj T* 0 Tw (many cores.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 363.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Effect of File Format) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 189.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 158 Tm /F1 10 Tf 12 TL 1.354104 Tw (Figures [] and [] show comparison of job execution time, total compute and I/O time averaged over all) Tj T* 0 Tw 1.784985 Tw (processes and the difference between these two times for 300X and 600X trajectories and for all file) Tj T* 0 Tw .052485 Tw (formats respectively. As can be seen, job execution time does not scale very well across parallelisms from) Tj T* 0 Tw .658935 Tw (1 to 72 for all formats. XTC and NCDF file formats reveals much better scaling as compared to DCD file) Tj T* 0 Tw .099988 Tw (format. As shown in Figure [], the results from different machines lie on top of each other for total compute) Tj T* 0 Tw .013488 Tw (and IO time for XTC and NCDF file formats; however, this is not the case for job execution time. Unlike job) Tj T* 0 Tw .163984 Tw (execution time, total compute and I/O time averaged over all processes reveals a reasonable scaling. The) Tj T* 0 Tw 1.123145 Tw (same behavior can be seen for other trajectory sizes as shown also in Figures 36 to 41. Based on the) Tj T* 0 Tw .32561 Tw (present result, there is a difference between job execution time, and total compute and I/O time averaged) Tj T* 0 Tw 1.89436 Tw (over all processes. This difference increases with increase in trajectory size for all file formats for all) Tj T* 0 Tw .129318 Tw (machines. This time difference is much smaller for Comet and Stampede as compared to other machines.) Tj T* 0 Tw 2.628221 Tw (In order to find the underlying reasons for this difference, web interface of Dask is used to obtain) Tj T* 0 Tw 3.05229 Tw (information about the amount of time spent on the communication between workers, and different) Tj T* 0 Tw (computations at the worker level in the Map-reduce job.) Tj T* ET
Q
Q
 
endstream
endobj
46 0 obj
<<
/Length 3073
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 747.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Challenges for Good HPC Performance) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 717.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Performance Optimization) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 690.0236 cm
q
BT 1 0 0 1 0 2.5 Tm 15 TL /F4 12.5 Tf 0 0 0 rg (Effect of Striping) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 663.0236 cm
q
BT 1 0 0 1 0 2.5 Tm 15 TL /F4 12.5 Tf 0 0 0 rg (Effect of Oversubscribing) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 636.0236 cm
q
BT 1 0 0 1 0 2.5 Tm 15 TL /F4 12.5 Tf 0 0 0 rg (Examining Scheduler Overhead) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 609.0236 cm
q
BT 1 0 0 1 0 2.5 Tm 15 TL /F4 12.5 Tf 0 0 0 rg (Scheduler Plugin Results) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 561.0236 cm
q
BT 1 0 0 1 0 21 Tm 18 TL /F2 15 Tf 0 0 0 rg (Comparison of Performance of Map-Reduce Job Between MPI for) Tj T* (Python and Dask Frameworks) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 531.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (References) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 468.3307 cm
Q
q
1 0 0 1 62.69291 439.9843 cm
n 0 14.17323 m 469.8898 14.17323 l S
Q
q
1 0 0 1 62.69291 355.9843 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 60 cm
q
BT 1 0 0 1 0 14 Tm 12 TL /F1 10 Tf 0 0 0 rg (Gowers2016\() Tj 0 0 .501961 rg (1) Tj 0 0 0 rg (,) Tj T* 0 0 .501961 rg (2) Tj 0 0 0 rg (, ) Tj 0 0 .501961 rg (3) Tj 0 0 0 rg (\)) Tj T* ET
Q
Q
q
1 0 0 1 91.03937 78 cm
Q
q
1 0 0 1 91.03937 78 cm
Q
q
1 0 0 1 91.03937 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 63 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 4 0 Td (R.) Tj T* -4 0 Td ET
Q
Q
q
1 0 0 1 23 69 cm
Q
q
1 0 0 1 23 69 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 57 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 6.22 0 Td (J.) Tj T* -6.22 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 62 Tm .366693 Tw 12 TL /F1 10 Tf 0 0 0 rg (Gowers, M. Linke, J. Barnoud, T. J. E. Reddy, M. N. Melo, S. L. Seyler, D.) Tj T* 0 Tw 1.72549 Tw (L. Dotson, J. Doman) Tj /F5 10 Tf 12 TL (n) Tj /F1 10 Tf 12 TL (ski, S. Buchoux, I. M. Kenney, and O. Beckstein.) Tj T* 0 Tw 4.563377 Tw (MDAnalysis: A Python package for the rapid analysis of molecular) Tj T* 0 Tw .733377 Tw (dynamics simulations. In S. Benthall and S. Rostrup, editors, Proceedings) Tj T* 0 Tw 1.147533 Tw (of the 15th Python in Science Conference, pages 102 \226 109, Austin, TX,) Tj T* 0 Tw (2016. SciPy. URL ) Tj 0 0 .501961 rg (http://mdanalysis.org) Tj 0 0 0 rg (.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 91.03937 0 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 319.9843 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 12 cm
q
BT 1 0 0 1 0 14 Tm 12 TL /F1 10 Tf 0 0 0 rg (Khoshlessan201) Tj T* (7) Tj T* ET
Q
Q
q
1 0 0 1 91.03937 0 cm
q
BT 1 0 0 1 0 26 Tm .845599 Tw 12 TL /F1 10 Tf 0 0 0 rg (Khoshlessan, Mahzad; Beckstein, Oliver \(2017\): Parallel analysis in the MDAnalysis) Tj T* 0 Tw 21.92673 Tw (Library: Benchmark of Trajectory File Formats. figshare.) Tj T* 0 Tw (doi:) Tj 0 0 .501961 rg (10.6084/m9.figshare.4695742) Tj T* ET
Q
Q
q
Q
Q
 
endstream
endobj
47 0 obj
<<
/Nums [ 0 48 0 R 1 49 0 R 2 50 0 R 3 51 0 R ]
>>
endobj
48 0 obj
<<
/S /D /St 1
>>
endobj
49 0 obj
<<
/S /D /St 2
>>
endobj
50 0 obj
<<
/S /D /St 3
>>
endobj
51 0 obj
<<
/S /D /St 4
>>
endobj
xref
0 52
0000000000 65535 f 
0000000073 00000 n 
0000000147 00000 n 
0000000254 00000 n 
0000000366 00000 n 
0000000541 00000 n 
0000000716 00000 n 
0000000890 00000 n 
0000001063 00000 n 
0000001231 00000 n 
0000001478 00000 n 
0000001652 00000 n 
0000001821 00000 n 
0000001996 00000 n 
0000002100 00000 n 
0000002339 00000 n 
0000002508 00000 n 
0000002733 00000 n 
0000002853 00000 n 
0000003021 00000 n 
0000003190 00000 n 
0000003359 00000 n 
0000003475 00000 n 
0000003649 00000 n 
0000003845 00000 n 
0000004098 00000 n 
0000004204 00000 n 
0000004478 00000 n 
0000004553 00000 n 
0000004751 00000 n 
0000004860 00000 n 
0000004982 00000 n 
0000005109 00000 n 
0000005249 00000 n 
0000005385 00000 n 
0000005535 00000 n 
0000005713 00000 n 
0000005833 00000 n 
0000005973 00000 n 
0000006116 00000 n 
0000006242 00000 n 
0000006443 00000 n 
0000006555 00000 n 
0000006636 00000 n 
0000011497 00000 n 
0000017571 00000 n 
0000023508 00000 n 
0000026633 00000 n 
0000026701 00000 n 
0000026735 00000 n 
0000026769 00000 n 
0000026803 00000 n 
trailer
<<
/ID 
[<1e5de8fb14b4659600d3c2eac3d3a244><1e5de8fb14b4659600d3c2eac3d3a244>]
% ReportLab generated PDF document -- digest (http://www.reportlab.com)

/Info 26 0 R
/Root 25 0 R
/Size 52
>>
startxref
26837
%%EOF
