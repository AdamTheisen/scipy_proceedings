.. -*- mode: rst; mode: visual-line; fill-column: 9999; coding: utf-8 -*-

:author: Mahzad Khoshlessan
:email: mkhoshle@asu.edu
:institution: Arizona State University, Department of Physics, Tempe, AZ 85287, USA

:author: Ioannis Paraskevakos
:email: i.paraskev@rutgers.edu
:institution: RADICAL, ECE, Rutgers University, Piscataway, NJ 08854, USA

:author: Shantenu Jha
:email: shantenu.jha@rutgers.edu
:institution: RADICAL, ECE, Rutgers University, Piscataway, NJ 08854, USA

:author: Oliver Beckstein
:email: obeckste@asu.edu 
:institution: Arizona State University, Department of Physics, Tempe, AZ 85287, USA 
:corresponding:

:bibliography: ``mdanalysis``


.. STYLE GUIDE
.. ===========
.. .
.. Writing
..  - use past tense to report results
..  - use present tense for intro/general conclusions
.. .
.. Formatting
..  - restructured text
..  - hard line breaks after complete sentences (after period)
..  - paragraphs: empty line (two hard line breaks)
.. .
.. Workflow
..  - use PRs (keep them small and manageable)

.. definitions (like \newcommand)

.. |Calpha| replace:: :math:`\mathrm{C}_\alpha`
.. |tcomp| replace:: :math:`t_\text{comp}`
.. |tIO| replace:: :math:`t_\text{I/O}`
.. |tcomptIO| replace:: :math:`t_\text{comp}+t_\text{I/O}`
.. |t_tot| replace:: :math:`t_\text{total}`
.. |S| replace:: :math:`\frac{t_{1}}{t_{N}}`
.. |E| replace:: :math:`\frac{S}{N}`
.. |avg_tcomp| replace:: :math:`\langle t_\text{compute} \rangle`
.. |avg_tIO| replace:: :math:`\langle t_\text{I/O} \rangle`
.. |Ncores| replace:: :math:`N_\text{cores}`

-------------------------------------------------------------------------
Parallel Analysis in MDAnalysis using the Dask Parallel Computing Library
-------------------------------------------------------------------------

.. class:: abstract

   The analysis of biomolecular computer simulations has become a challenge because the amount of output data is now routinely in the terabyte range.
   We evaluated if this challenge can be met by a parallel map-reduce approach with the Dask_ parallel computing library for task-graph based computing coupled with our MDAnalysis_ Python library for the analysis of molecular dynamics (MD) simulations.
   We performed a representative performance evaluation, taking into account the highly heterogeneous computing environment that researchers typically work in together with the diversity of existing file formats for MD trajectory data.
   We found that the underlying storage system (solid state drives, parallel file systems, or simple spinning platter disks) can be a deciding performance factor that leads to data ingestion becoming the primary bottleneck in the analysis work flow.
   However, the choice of the data file format can mitigate the effect of the storage system; in particular, the commonly used Gromacs XTC trajectory format, which is highly compressed, can exhibit strong scaling close to ideal due to trading a decrease in global storage access load against an increase in local per-core cpu-intensive decompression.
   Scaling was tested on a single node and multiple nodes on national and local supercomputing resources as well as typical workstations.
   In summary, we show that, due to the focus on high interoperability in the scientific Python eco system, it is straightforward to implement map-reduce with Dask in MDAnalysis and provide an in-depth analysis of the considerations to obtain good parallel performance on HPC resources.

.. class:: Keywords

   MDAnalysis, High Performance Computing, Dask, Map-Reduce, MPI for Python


Introduction
============

MDAnalysis_ is a Python library that provides users with access to raw simulation data and enables structural and temporal analysis of molecular dynamics (MD) trajectories generated by all major MD simulation packages :cite:`Gowers:2016aa, Michaud-Agrawal:2011fu`.
MD trajectories are time series of positions (and sometimes also velocities) of the simulated atoms or particles; using statistical mechanics one can calculate experimental observables from these time series :cite:`FrenkelSmit02,Mura:2014kx`.
The size of these trajectories is growing as the simulation times are being extended beyond micro-seconds and larger systems with increasing numbers of atoms are simulated.
The amount of data to be analyzed is growing rapidly into the terabyte range and analysis is increasingly becoming a bottleneck in MD workflows :cite:`Cheatham:2015qf`.
Therefore, there is a need for high performance computing (HPC) approaches for the analysis of MD trajectory data :cite:`Tu:2008dq, Roe:2013zr`.

MDAnalysis does not yet provide a standard interface for parallel analysis; instead, various existing parallel libraries such as Python multiprocessing_, joblib_, and mpi4py_ :cite:`Dalcin:2005aa,Dalcin:2011aa` are currently used to parallelize MDAnalysis-based code on a case-by-case basis.
Here we evaluated performance for parallel map-reduce :cite:`Dean:2008aa` type analysis with the Dask_ parallel computing library :cite:`Rocklin:2015aa` for task-graph based distributed computing on HPC and local computing resources.
Although Dask is able to implement much more complex computations than map-reduce, we chose Dask for this task because of its ease of use and because we envisage using this approach for more complicated analysis applications whose parallelization cannot easily be expressed as a simple map-reduce algorithm.

.. figure:: figs/panels/rmsd_dask.pdf
   :figclass: b

   rmsd calculation via map-reduce with Dask.
   **A** rmsd as a function of time, with partial time series colored by trajectory block.   
   **B** Dask task graph for splitting the rmsd calculation into three trajectory blocks.
   :label:`rmsd-dask`

As the computational task we performed an optimal structural superposition of the atoms of a protein to a reference structure by minimizing the RMSD of the |Calpha| atoms :cite:`Mura:2014kx` (Figure :ref:`rmsd-dask`).
A range of commonly used MD file formats (CHARMM/NAMD DCD :cite:`Brooks:2009pt`, Gromacs XTC :cite:`Abraham:2015aa`, Amber NetCDF (NCDF) classic format version 3.6.0 :cite:`Case:2005uq`) and different trajectory sizes were benchmarked.

We looked at different HPC resources including national supercomputers (XSEDE TACC *Stampede* and SDSC *Comet*), university supercomputers (Arizona State University Research Computing *Saguaro*), and local resources (Gigabit networked multi-core workstations). 
The tested resources are parallel and heterogeneous with different CPUs, file systems, high speed networks and are suitable for high-performance distributed computing at various levels of parallelization.
Different storage systems such as solid state drives (SSDs), hard disk drives (HDDs), and the parallel Lustre file system (implemented on top of HDD) were tested to examine the effect of I/O on the performance. 
The benchmarks were performed both on a single node and across multiple nodes using the multiprocessing and distributed_ schedulers in the Dask library.

Our results showed strong dependency on the storage system because competition for access to the same file from multiple processes emerged as a key problem.
But because the trajectory file format dictates the data access pattern, the overall performance depends critically on the storage system *and* the actual data format, with some formats being more robust against storage system specifics than others.

Overall, good performance and strong scaling with the number of CPU cores was found on a single node but robust across-node performance remained challenging.
In order to identify performance bottlenecks we examined several other factors including the effect of striping in the parallel Lustre file system, oversubscribing (using more tasks than Dask workers), the performance of the Dask scheduler itself, and we also benchmarked an MPI-based implementation in contrast to the Dask approach.
From these tests we tentatively conclude that poor across-nodes performance is rooted in contention on the shared network that may slow down individual tasks and lead to poor load balancing.
Nevertheless, Dask with MDAnalysis appears to be a promising approach for high-level parallelization for analysis of MD trajectories, especially at moderate CPU core numbers.


Methods
=======

We implemented a simple map-reduce scheme to parallelize processing of trajectories over contiguous blocks.
We tested libraries in the following versions: MDAnalysis 0.15.0, Dask 0.12.0 (also 0.13.0), Distributed_ 1.14.3 (also 1.15.1), and NumPy 1.11.2 (also 1.12.0) :cite:`VanDerWalt2011`.

.. code-block:: python

   import numpy as np
   import MDAnalysis as mda
   from MDAnalysis.analysis.rms import rmsd

The trajectory is split into ``n_blocks`` blocks with inital frame ``start`` and final frame ``stop``  set for each block.
The calculation on each block (function ``block_rmsd()``, corresponding to the *map* step) is *delayed* with the ``delayed()`` function in Dask:

.. code-block:: python

   from dask.delayed import delayed

   def analyze_rmsd(ag, n_blocks):
       """RMSD of AtomGroup ag, parallelized n_blocks"""
       ref0 = ag.positions.copy()
       bsize = int(np.ceil(
                   ag.universe.trajectory.n_frames \
                   / float(n_blocks)))
       blocks = []
       for iblock in range(n_blocks):
	   start, stop = iblock*bsize, (iblock+1)*bsize
	   out = delayed(block_rmsd, pure=True)(
	           ag.indices, ag.universe.filename,
		   ag.universe.trajectory.filename,
		   ref0, start, stop)   
	   blocks.append(out)
   return delayed(np.vstack)(blocks)

In the *reduce* step, the partial time series from each block are concatenated in the correct order (``np.vstack``, see Figure :ref:`rmsd-dask` A); because results from delayed objects are used, this step also has to be delayed.

As computational load we implement the calculation of the root mean square distance (rmsd) of the |Calpha| atoms of the protein adenylate kinase :cite:`Seyler:2014il` when fitted to a reference structure using an optimal rigid body superposition :cite:`Mura:2014kx`, using the qcprot implementation :cite:`PuLiu_FastRMSD_2010` in MDAnalysis :cite:`Gowers:2016aa`.
The rmsd is calculated for each trajectory frame in each block by iterating over ``u.trajectory[start:stop]``:

.. code-block:: python

   def block_rmsd(index, topology, trajectory, ref0,
                  start, stop):
       u = mda.Universe(topology, trajectory)
       ag = u.atoms[index]
       out = np.zeros([stop-start, 2])
       for i, ts in enumerate(
               u.trajectory[start:stop]):
	   out[i, :] = ts.time, rmsd(ag.positions, ref0,
	                 center=True, superposition=True)
       return out

Dask produces a task graph (Figure :ref:`rmsd-dask` B) and the computation of the graph is executed in parallel through a Dask scheduler such as ``dask.multiprocessing`` (or ``dask.distributed``):

.. code-block:: python

   from dask.multiprocessing import get

   u = mda.Universe(PSF, DCD)
   ag = u.select_atoms("protein and name CA")
   result = analyze_rmsd(ag, n_blocks)
   timeseries = result.compute(get=get)


The complete code for benchmarking is available from https://github.com/Becksteinlab/Parallel-analysis-in-the-MDAnalysis-Library under the MIT License.

The data files consist of a topology file ``adk4AKE.psf`` (in CHARMM PSF format; :math:`N = 3341` atoms) and a trajectory ``1ake_007-nowater-core-dt240ps.dcd`` (DCD format) of length 1.004 µs with 4187 frames; both are freely available from figshare at DOI `10.6084/m9.figshare.5108170`_  :cite:`Seyler:2017aa`.
Files in XTC and NCDF formats are generated from the DCD on the fly using MDAnalysis.
To avoid operating system caching, files were copied and only used once for each benchmark.
All results for Dask distributed were obtained across three nodes on different clusters.

Trajectories with different number of frames per trajectory were analyzed to assess the effect of trajectory file size.
These trajectories were generated by concatenating the base trajectory 50, 100, 300, and 600 times and are referred to as, e.g., "DCD300x" or "XTC600x".
For Dask multiprocessing we investigated 50x, 100x, 300x and 100x, 300x, and 600x for Dask distributed; however, here we only present data for the 300x and 600x trajectory sizes, which represent typical medium size results.
The DCD file format is just a binary representation and the DCD300x trajectory has a file size of 47 GB (DCD600x is twice as much); XTC is a lossy compressed format and XTC300x is only 15 GB; NCDF is about the same size as DCD.
Speed up (|S|), efficiency (|E|), IO time per frame (|tIO|), computation time for completing RMSD task per frame (|tcomp|), Job execution time (|t_tot|), and total time for completing IO and RMSD calculation for all frames per process (|tcomptIO|) are the measured quantities for analyzing the present benchmark.
For an analysis of the full data set see the Technical Report :cite:`Khoshlessan:2017aa`.


Results and Discussion
======================

Effect of File Format on I/O Time
---------------------------------

In MDAnalysis library, trajectories from MD simulations are a frame by frame description of the motion of particles as a function of time. 
To allow the analysis of large trajectories, MDAnalysis only loads a single frame into memory at any time :cite:`Gowers:2016aa, Michaud-Agrawal:2011fu`.
Depending on the file format the loading time of frames into memory will be different.
Some file systems like distributed parallel file systems (Lustre) allow simultaneous access to the file by different processes; however this will be possible only if there is a parallel I/O library which is not the case in the present study.
Figure :ref:`pattern-formats` illustrates the I/O pattern compared between different file formats.
Figure :ref:`IO-comparison` compares the difference in I/O time for different file formats for 300X trajectory for multiprocessing (A, B, C) and distributed (D, E, F) schedulers respectively. 

.. figure:: figs/panels/trj-access-patterns.pdf

   I/O pattern for reading frames in parallel from commonly used MD trajectory formats.
   **A** Gromacs XTC file format.
   **B** CHARMM/NAMD DCD file format and Amber NCDF format.
   :label:`pattern-formats`

XTC file format takes advantage of in-built compression and as a result has smaller file size as compared to the other formats. 
In addition, MDAnalysis implements a fast frame scanning algorithm for XTC files.
This algorithm computes frame offsets and saves the offsets to disk as a hidden file once the trajectory is read the first time. 
When a trajectory is loaded again then instead of reading the whole trajectory the offset is used to seek individual frames. 
As a result, opening the same file again is fast. 
For XTC file format, each frame I/O will be followed by decompressing of that frame as soon as it is loaded into memory (see Figure :ref:`pattern-formats` A). 
Thus, as soon as the frame is loaded into memory by one process, the file system will let the next process to load its requested frame into memory.
This happens while the first process is decompressing the loaded frame.
As a result, the overlapping of the requests to different frames by different processes will be less frequent.
This is why IO time per frame for XTC file format remains level with increasing the number of processes for both schedulers (Figure :ref:`IO-comparison` B, E).
However, DCD and NCDF file formats do not take benefit from in-built compression and as a result their file sizes are larger as compared to XTC file format.
The IO pattern for DCD and NCDF file format is shown in Figure :ref:`pattern-formats` B. 
As seen the piplining of the frame access is not happeing for these file formats and other processes are prevented from accessing their requested frame while another process is loading its frame into memory. 
The overlapping of per frame trajectory data access can lead to higher IO time.
The overlapping of per frame trajectory data access is especially critical when defined tasks per process do not have the desired level of granularity which is the case in the present benchmark. 
DCD file format has a very simple format and the IO time per frame is very small as compared to other formats when the number of processes is small.
As the number of processes increases, IO time per frame increases due to the overlapping of per frame trajectory data access (Figure :ref:`IO-comparison` A, D). 
According to Figure :ref:`IO-comparison` A, SSD can be very helpful for DCD file formats and can lead to significant improvement in performance due to faster access time.

The I/O time per frame is larger for NCDF file format as compared to DCD file format due to larger file size (Figure :ref:`IO-comparison` C, F).
Also, NCDF has a more complicated file format. 
Reading an existing NCDF data set involves opening the data set, inquiring about dimensions, variables and attributes, reading variable data, and closing the data set.
The NCDF format is more sophisticated than the DCD format, which might contribute to the better scaling of parallel access to NCDF files than to DCD files.
This is why IO time per frame remains level up to higher number of cores for NCDF file format (Figure :ref:`IO-comparison` C, F). 

Figure :ref:`time-comparison` compares job execution time between different file format for 300x trajectory sizes using Dask multiprocessing and distributed schedulers.
According to Figure :ref:`time-comparison` A, DCD files which are single precision binary FORTRAN files and have a simpler format as compared to XTC and NCDF are faster and have less execution time especially using SSDs.
As described above, IO time per frame remains pretty level for DCD file format using SSDs (Figure :ref:`IO-comparison` A) and as a result DCD file format shows a very good scaling on a single node.
Moreover, job execution time for DCD file format using SSDs is one order of magnitude smaller than other formats.
In fact, reducing IO time can lead to noticeable improvement in performance which emphasizes the impact of IO time on the overall performance.
According to the present benchmark, one can achieve a very good speed up using many SSDs for DCD file format on a single node.
Based on Figure :ref:`IO-comparison` A, B and C, very good speed up is achievable using SSDs for DCD format in much shorter time as compared to XTC and NCDF file formats.
One can also achieve better performance with DCD file format by increasing the level of granularity per process.
XTC and NCDF have comparable larger execution time as compared to DCDs due to their rather more complex file formats than DCDs (Figure :ref:`time-comparison` B, C, E, F).

.. figure:: figs/panels/IO-time-comparison.pdf

   Comparison of IO time between different file formats for 300x trajectory sizes using Dask multiprocessing on a *single node* (A, B, C) and Dask distributed (D, E, F).
   The trajectory was split into :math:`N` blocks and computations were performed using :math:`N_\text{cores} = N` CPU cores.
   The runs were performed on different resources (ASU RC *Saguaro*, SDSC *Comet*, TACC *Stampede*, *local* workstations with different storage systems (locally attached *HDD*, *remote HDD* (via network file system), locally attached *SSD*, *Lustre* parallel file system with a single stripe).
   :label:`IO-comparison`

.. figure:: figs/panels/timing-comparison.pdf

   Comparison of job execution time between 300x trajectory sizes using Dask multiprocessing on a *single node* (A, B, C) and Dask distributed (D, E, F).
   The trajectory was split into :math:`N` blocks and computations were performed using :math:`N_\text{cores} = N` CPU cores.
   The runs were performed on different resources (ASU RC *Saguaro*, SDSC *Comet*, TACC *Stampede*, *local* workstations with different storage systems (locally attached *HDD*, *remote HDD* (via network file system), locally attached *SSD*, *Lustre* parallel file system with a single stripe).
   :label:`time-comparison`


Performance Comparison between Different File Format
----------------------------------------------------

Figure :ref:`speedup-300x` shows speed up comparison for 300x trajectories between multiprocessing and distributed schedulers.
The DCD file format does not scale at all by increasing parallelism across different cores (Figure :ref:`speedup-300x` A, D).
This is due to the fact that IO time does not remain level by increasing the number of processes as discussed in the previous section.
Our study showed that SSDs can be very helpful and can lead to better performance for all file formats especially DCD file format (Figure :ref:`speedup-300x` A, D).  
XTC file format expresses reasonably well scaling with the increase in parallelism up to the limit of 24 (single node) for both multiprocessing and distributed scheduler.
The NCDF file format scales very well up to 8 cores for all trajectory sizes.
As the number of prcocesses increases the IO time also increases for NCDF file format and as a result the scaling is limited up to 8 CPU cores.
For XTC file format, the I/O time is leveled up to 50 cores and compute time also remains level across parallelism up to 72 cores.
Therefore, it is expected to achieve speed up, across parallelism up to 50 cores.
However, based on Figure :ref:`speedup-300x` E, XTC format only scales well up to 20 cores.
Based on the present result, there is a difference between job execution time, and total compute and I/O time averaged over all processes (Figure :ref:`timing-XTC-600x` A).
This difference increases with increase in trajectory size for all file formats for all machines (For details refer to the Technical Report :cite:`Khoshlessan:2017aa`).
This time difference is much smaller for Comet and Stampede as compared to other machines.
The difference between job execution time and total compute and I/O time measured inside our code is very small for the results obtained using multiprocessing scheduler; however, it is considerable for the results obtained using distributed scheduler.

In order to obtain more insight on the underlying network behavior both at the worker level and communication level and in order to be able to see where this difference originates from we have used the web-interface of the Dask library.
This web-interface is launched whenever Dask scheduler is launched.
Figure :ref:`task-stream-comet` B, summarizes the measured |tcomptIO| averaged over all processes, max |tcomptIO|, and Job execution time for XTC600x on SDSC Comet for two different CPU cores. 
For :math:`N_\text{cores} = 54`, the measured max |tcomptIO| through our infrasturacture and web-interface show two different values. 
The max |tcomptIO| measured using Dask web-interface is closer to the measured job execution time. 
The reason why max |tcomptIO| measured using Dask web-interface and our infrastructure is open to question.
Based on Figure :ref:`task-stream-comet` A, there is one process which is much slower as compared to others. 
Therefore, because some tasks (so-called Stragglers) are considerably slower than the others, the job is delayed and as a result the overall performance is affected.

.. figure:: figs/panels/speedup-comparison.pdf

   Speed-up for the analysis of the 300x trajectory on HPC resources using Dask multiprocessing and distributed.
   The dashed line shows the ideal limit of strong scaling.
   The runs were performed on different resources (ASU RC *Saguaro*, SDSC *Comet*, TACC *Stampede*, all using Lustre with a single stripe as the parallel file system and *local* workstations with NFS).
   **A**, **D** CHARMM/NAMD DCD.
   **B**, **E** Gromacs XTC.
   **C**, **F** Amber NCDF.
   :label:`speedup-300x`

.. figure:: figs/panels/timing-XTC-600x.pdf

   Timings for various parts of the computation for the 600x XTC trajectory on HPC resources using Dask distributed.
   The runs were performed on different resources (ASU RC *Saguaro*, SDSC *Comet*, TACC *Stampede*, all using Lustre with a single stripe as the parallel file system and  *local* workstations with NFS).
   **A** Total time to solution (wall clock), :math:`t_N` for :math:`N` trajectory blocks using :math:`N_\text{cores} = N` CPU cores.
   **B** Sum of the measured I/O time |tIO| and the (constant) time for the RMSD computation |tcomp| (data not shown).
   **C** Difference :math:`t_N - (t_\text{I/O} + t_\text{comp})`, accounting for other load that is not directly measured.
   :label:`timing-XTC-600x`

   
.. figure:: figs/XTC600-54c-Web-In-Comet.pdf
   
   Summary of measured times tested on SDSC Comet using :math:`N_\text{cores} = 54` for 600X trajectory and XTC file format
   **A** Task stream plots showing the fraction of time spent on different computations by each worker obtained using Dask web-interface-Green bars represent time spent on RMSD calculations.
   **B** Timing comparison between present infrastructure and Dask web-interface for :math:`N_\text{cores} = 30` and :math:`N_\text{cores} = 54` cores.
   :label:`task-stream-comet`


Challenges for Good HPC Performance
-----------------------------------

It should be noted that all the present results were obtained during normal, multi-user, production periods on all machines.
In fact, the time the jobs take to run is affected by the other jobs on the system.  
This is true even when the job is the only one using a particular node, which was the case in the present study.  
There are shared resources such as network file systems that all the nodes use.  
The high speed interconnect that enables parallel jobs to run is also a shared resource.  
The more jobs are running on the cluster, the more contention there is for these resources.  
As a result, the same job runs at different times will take a different amount of time to complete.  
In addition, remarkable fluctuations in task completion time across different processes is observed through monitoring network behavior using Dask web-interface.  
These fluctuations differ in each repeat and are dependent on the hardware and network. 
These factors further complicate any attempts at benchmarking. 
Therefore, this makes it really hard to optimize codes, since it is hard to determine whether any changes in the code are having a positive effect.
This is because the margin of error introduced by the non-deterministic aspects of the cluster's environment is greater than the performance improvements the changes might produce.
There is also variability in network latency, in addition to the variability in underlying hardware in each machine.
This causes the results to vary significantly across different machines.
Since our Map-reduce job is pleasantly parallel, all of our processes have the same amount of work to do and our Map-Reduce job is load balanced. 
Therefore, observing these stragglers discussed in the previous section is unexpected and the following sections in the present study aim to identify the reason for which we are seeing these stragglers.

Performance Optimization
------------------------

In the present section, we have tested different features of our computing environment to see if we can identify the reason for those stragglers and improve performance by avoiding the stragglers.
Lustre striping, oversubscribing, scheduler throughput are tested to examine their effect on the performance. 
In addition, scheduler plugin is also used to validate our observations from Dask web-interface.
In fact, we create a plugin that performs logging whenever a task changes state.
Through the scheduler plugin we will be able to get lots of information about a task whenever it finishes computing.

Effect of Lustre Striping
~~~~~~~~~~~~~~~~~~~~~~~~~

As discussed before, the overlapping of data requests from different processes can lead to higher I/O time and as a result poor performance.
This is strongly affecting our results since our compute per frame is not heavy and therefore the overlapping of data requests will be more frequent depending on the file format.
The effect on the performance is strongly dependent on file format and some formats like XTC file formats which take advantage of in-built decompression are less affected by the contention from many data requests from many processes.
However, when extending to more than one node, even XTC files are affected by this, as is also shown in the previous sections.
In Lustre, a copy of the shared file can be in different physical storage devices (OSTs). 
Single shared files can have a stripe count equal to the number of nodes or processes which access the file.
In the present study, we set the stripe count equal to three which is equal to the number of nodes used for our benchmark.
This may be helpful to improve performance, since all the processes from each node will have a copy of the file and as a result the contention due to many data requests will decrease.
Figure :ref:`speedup-IO-600x-striping` show the speed up and I/O time per frame plots obtained for XTC file format (600X) when striping is activated. 
As can be seen, IO time is level across parallelism up to 72 cores which means that striping is helpful for leveling IO time per frame across all cores.
However, based on the timing plots shown in Figure :ref:`timing-600x-striping`, there is a time difference between average total compute and I/O time and job execution time which is due to the stragglers and as a result the overall speed-up is not improved as compared to what we had in Figure :ref:`speedup-600x`.  

.. figure:: figs/panels/speed-up-IO-600x-striping.pdf

   **A** Speed-up for the analysis of the 600x trajectory on HPC resources using Dask distributed with strip count of three.
   The dashed line shows the ideal limit of strong scaling.
   **B** Comparison of IO time between 600x trajectory sizes using Dask distributed on one to three nodes.
   The runs were performed on different resources (ASU RC *Saguaro*, SDSC *Comet*, all using Lustre with strip count of three as the parallel file system).
   :label:`speedup-IO-600x-striping`


.. figure:: figs/panels/timing-XTC-600x-striping.pdf
   
   Timings for various parts of the computation for the 600x XTC trajectory on HPC resources using Dask distributed.
   The runs were performed on different resources (ASU RC *Saguaro*, SDSC *Comet*, all using Lustre with stripe count of three as the parallel file system and *local* workstations with NFS).
   **A** Total time to solution (wall clock), :math:`t_N` for :math:`N` trajectory blocks using :math:`N_\text{cores} = N` CPU cores.
   **B** Sum of the measured I/O time |tIO| and the (constant) time for the RMSD computation |tcomp| (data not shown).
   **C** Difference :math:`t_N - (t_\text{I/O} + t_\text{comp})`, accounting for other load that is not directly measured.
   :label:`timing-600x-striping`


Effect of Oversubscribing
~~~~~~~~~~~~~~~~~~~~~~~~~

One useful way to robust our code to uncertainty in computations is to submit much more tasks than the number of cores. 
This may allow Dask to load balance appropriately, and as a result cover the extra time when there are some stragglers.
In order for this, we set the number of tasks to be three times the number of workers (:math:`N_\text{Blocks} = 3*N_\text{cores}`). 
Striping is also activated and is set to three which is also equal to number of nodes.
Figures :ref:`speedup-IO-600x-oversubscribing` show the speed up, and I/O time per frame plots obtained for XTC file format (600X).
As can be seen, IO time is level across parallelism up to 72 cores which means that striping is helpful for leveling IO time per frame across all cores.
However, based on the timing plots shown in Figure :ref:`timing-600x-oversubscribing`, there is a time difference between average total compute and I/O time and job execution time which reveals that oversubscribing does not help to remove the stragglers and as a result the overall speed-up is not improved as compared to what we had in Figure :ref:`speedup-600x`.
Figure :ref:`Dask-time-stacked-comparison` shows time comparison on different parts of the calculations. 
Bars are subdivided into the contribution of overhead in the calculations, communication time and RMSD calculation across parallelism from 1 to 72.
RMSD calculation is the time spent on RMSD tasks, and communication time is the time spent for gathering RMSD arrays calculated by each processor rank.
As can be seen in Figure :ref:`Dask-time-stacked-comparison`, the overhead in the calculations is small up to 24 cores (Single node).
The largest fraction of the calculations is spent on the calculation of RMSD arrays (computation time) which decreases pretty well as the number of cores increases from 1 to 72.
However, when extending to multiple nodes the time due to overhead and communication increases which affects the overall performance.
The results from scheduler plugin is described in the following section.

.. figure:: figs/panels/speed-up-IO-600x-oversubscribing.pdf

   **A** Speed-up for the analysis of the 600x trajectory on HPC resources using Dask distributed with strip count of three.
   The dashed line shows the ideal limit of strong scaling.
   **B** Comparison of IO time between 600x trajectory sizes using Dask distributed on one to three nodes.
   The runs were performed on different resources (ASU RC *Saguaro*, SDSC *Comet*, and our local machines, all using Lustre with strip count of three as the parallel file system).
   :math:`N` number of blocks is three times the number of processes :math:`N = 3*N_\text{cores}`
   :label:`speedup-IO-600x-oversubscribing`

.. figure:: figs/panels/timing-XTC-600x-oversubscribing.pdf

   Timings for various parts of the computation for the 600x XTC trajectory on HPC resources using Dask distributed. The runs were performed on different resources (ASU RC *Saguaro*, SDSC *Comet*, and our local machines, all using Lustre with stripe count of three as the parallel file system and *local* workstations with NFS).
   **A** Total time to solution (wall clock), :math:`t_N` for :math:`N` trajectory blocks using :math:`N = 3*N_\text{cores}` CPU cores.
   **B** Sum of the measured I/O time |tIO| and the (constant) time for the RMSD computation |tcomp| (data not shown).
   **C** Difference :math:`t_N - (t_\text{I/O} + t_\text{comp})`, accounting for other load that is not directly measured.
   :label:`timing-600x-oversubscribing`

.. figure:: figs/Dask-time_stacked_comparison.pdf

   Time comparison on different parts of the calculations obtained using Dask distributed with strip count of three.
   In this aggregate view, the time spent on different
   parts of the calculation are combined for different number of processes tested.
   The bars are subdivided into the contributions of each time spent on different parts.
   RMSD calculation is the time spent on RMSD task, and communication time and overhead is the time spent for gathering RMSD arrays calculated by each processor rank.
   and the overhead in the calculations that might had been caused due to different reasons.
   :math:`N` number of blocks is three times the number of processes :math:`N = 3*N_\text{cores}` and the results are for SDSC *Comet*.
   Reported values are the mean values across 5 repeats. :label:`Dask-time-stacked-comparison`

Examining Scheduler Throughput
------------------------------

An experiment were executed with Dask Schedulers (multithreaded, multiprocessing and distributed) on Stampede.
In each run a total of 100000 zero workload tasks were executed.
Figure :ref:`daskThroughput` A shows the Throughput of each Scheduler over time on a single Stampede node - Dask scheduler and worker are on the same node.
Each value is the mean throughput value of several runs for each Scheduler. 

.. figure:: figs/panels/daskThroughputPanel.pdf
   :scale: 30%

   Benchmark of Dask scheduler throughput on TACC *Stampede*.
   Performance is measured by the number of empty ``pass`` tasks that were executed in a second.
   The scheduler had to lauch 100,000 tasks and the run ended when all tasks had been run.
   **A** single node with different schedulers; multithreading and multiprocessing are almost indistinguishable from each other.
   **B** multiple nodes with the distributed scheduler and 1 worker process per node.
   **C** multiple nodes with the distributed scheduler and 16 worker processes per node.
   :label:`daskThroughput`

Our understanding is that the most efficient Scheduler is the distributed scheduler, especially when there is one worker process for each available core.
Also, the distributed with just one worker process and a number of threads equal to the number of available cores are still able to schedule and execute these 100,000 tasks.
The multiprocessing and multithreading schedulers have similar behavior again, but need significantly more time to finish compared to the distributed.

Figure :ref:`daskThroughput` B shows the distributed scheduler's throughput over time when the number of Nodes increases.
Each node has a single worker process and each worker launches a thread to execute a task (maximum 16 threads per worker).
By increasing the number of nodes we can see that Dask's throughput increases by the same factor. 
Figure :ref:`daskThroughput` C shows the same execution with the Dask Cluster being setup to have one worker process per core.
In this figure, the Scheduler does not reach its steady throughput state, compared to :ref:`daskThroughput` B, thus it is not clear what is the effect of the extra nodes.
Another interesting aspect is that when a worker process is assigned to each core, Dask's Throughput is an order of magnitude larger allowing for even faster scheduling decisions and task execution.

 
Scheduler Plugin Results
------------------------

In addition to Dask web-interface, we implemented a Dask scheduler plugin_.
This plugin captures task execution events from the scheduler and their respective timestamps.
These captured profiles were later used to analyze the execution of XTC 300x on Stampede.
In all the previous benchmarks in the present study, number of blocks is equal to the number of processes (:math:`N = N_\text{cores}`). 
However, when extended to multiple nodes the whole calculation is delayed due to the stragglers and as a result the overall performance was affected.
In the present section, we repeated the benchmark where the number of blocks is three times the number of processes (:math:`N =3*N_\text{cores}`).
We were able to measure how many tasks are submitted per worker process.
This exexutions are performed to see why oversubscribing introduced in the previous section was not helpful.
Table :ref:`process-subm` summarizes the results and Figure :ref:`task-histograms` shows in detail how RMSD blocks were submitted per worker process in each run.
As it is shown the execution is not balanced between worker processes.
Although, most workers are calculating three RMSD blocks, as it is expected by oversubscribing, there are a few workers that are receiving a smaller number of blocks and workers that receive more than three.
Therefore, we can conclude that over-subscription does not necessarily lead to a balanced execution, adding additional execution time.

.. table:: Summary of the number of worker processes per submitted RMSD blocks. Each column shows the total number of Worker process that executed a number of RMSD blocks per run. Executed on TACC Stampede utilizing 64 cores :label:`process-subm` 

   +------------+-------+-------+-------+-------+-------+
   |RMSD Blocks | Run 1 | Run 2 | Run 3 | Run 4 | Run 5 |
   +============+=======+=======+=======+=======+=======+
   |    1       |   0   |   0   |   1   |   0   |   0   |
   +------------+-------+-------+-------+-------+-------+
   |    2       |   8   |   5   |   7   |   7   |   2   |
   +------------+-------+-------+-------+-------+-------+
   |    3       |  48   |  54   |  56   |  50   |  60   |
   +------------+-------+-------+-------+-------+-------+
   |    4       |   8   |   5   |   0   |   7   |   2   |
   +------------+-------+-------+-------+-------+-------+

.. figure:: figs/x300TaskHistograms.pdf
   :figclass: w
   :scale: 50%
      
   Task Histogram of RMSD with MDAnalysis and Dask with XTC 300x over 64 cores on Stampede with 
   192 blocks. Each histogram is a different run of the same execution. The X axis is worker process ID and the Y     
   axis the number of tasks submitted to that process. :label:`task-histograms`


Comparison of Performance of Map-Reduce Job Between MPI for Python and Dask Frameworks
--------------------------------------------------------------------------------------

Based on the results presented in previous sections, it turned out that the stragglers are not because of the scheduler throughput.
Lustre striping improves I/O time; however, the job computation is still delayed due to stragglers and as a result performance was not improved.    
In order to make sure if the stragglers are created because of scheduler overhead in Dask framework we have tried to measure the performance of our Map-Reduce job using an MPI-based implementation, which makes use of mpi4py_ :cite:`Dalcin:2005aa,Dalcin:2011aa`.
This will let us figure out whether the stragglers observed in the present benchmark using Dask parallel library are as a result of scheduler overhead or any other factor than scheduler.
The comparison is performed on XTC 600x using SDSC Comet. 
Figure :ref:`MPItimestackedcomparison` shows time comparison on different parts of the calculations.
Bars are subdivided into the contribution of overhead in the calculations, communication time and RMSD calculation across parallelism from 1 to 72.
Computation time is the time spent on RMSD tasks, and communication time is the time spent for gathering RMSD arrays calculated by each processor rank.
Total time is the summation of communication time, computation time and the overhead in the calculations.
As can be seen in Figure :ref:`MPItimestackedcomparison`, the overhead in the calculations is small up to 24 cores (Single node).
Based on Figure :ref:`MPItimestackedcomparison`, the communication time is very small up to a single node and increases as the calculations are extended to multiple nodes. 
Overall, only a small fraction of total time is spent on communications.
Overhead in the calculations is also very small.
The largest fraction of the calculations is spent on the calculation of RMSD arrays (computation time) which decreases pretty well as the number of cores increases for a sigle node.
However, when extending to multiple nodes computation time also increases.
We believe that this is caused due to stragglers which is also confirmed based on Figure :ref:`MPI-total-time-rank-comparison`.

.. figure:: figs/MPItimestackedcomparison.pdf

   Time comparison on different parts of the calculations obtained using MPI for python. In this aggregate view, the time spent on different
   parts of the calculation are combined for different number of processes tested.
   The bars are subdivided into the contributions of each time spent on different parts.
   Computation time is the time spent on RMSD tasks, and communication time is the time spent for gathering RMSD arrays calculated by each processor rank.
   Total time is the summation of communication time, computation time and the overhead in the calculations that
   might had been caused due to different reasons.
   Reported values are the mean values across 5 repeats. 
   Total job execution time along with the mean and standard deviations across 5 repeats across parallelism from 1 to 72 obtained using MPI for python.
   The calculations are performed on XTC 600x using SDSC Comet.
   :label:`MPItimestackedcomparison`

Figure :ref:`MPItimestackedcomparison` shows job execution time along with the mean and standard deviations across 5 repeats across parallelism from 1 to 72.
Again, from Figure :ref:`MPItimestackedcomparison` job execution time reveals a decent decrease up to 24 cores (Single node).
However, when extended to multiple nodes the uncertainties in measured job execution time across different repeats increases and as a result job execution time increases as well.

Figure :ref:`MPItimestackedcomparison` shows comparison of job execution time across all ranks tested with 72 cores.
As seen in Figure :ref:`MPItimestackedcomparison` there are several slow processes as compared to others which slow down the whole process and as a result affect the overall performance. 
These stragglers are observed in all cases when number of cores is more than 24 (extended to multiple cores).
However, they are only shown for :math:`N = 72` CPU cores for the sake of brevity. 
 
Overall speed-up along with the efficiency plots are shown in Figure :ref:`MPI-Speed-up`.
As seen the overall performance is affected when extended to multiple nodes (more than 24 CPU cores). 

.. figure:: figs/panels/MPI-Speed-up.pdf

   **A** Speed-up and **B** efficiency plots for benchmark performed on XTC 600x on SDSC Comet across parallelism from 1 to 72 using MPI for python.
   Five repeats are run for each block size to collect statistics and the reported values are the mean values across 5 repeats.
   :label:`MPI-Speed-up`

Based on the results from MPI for python the reason for stragglers is not the Dask scheduler overhead.
In order to make sure that the reason for stragglers is not the qcprot RMSD calculation we tested the performance of our code using another metric `MDAnalysis.lib.distances.distance_array`_.
This metric calculates all distances between a reference set and another configuration.
Even with the new metric the same behavior observed and hence we can conclude that qcprot RMSD calculation is not the reason why we are seeing the stragglers.
Further studies are necessary to identify the underlying reason for the stragglers observed in the present benchmark.

Conclusions
===========

In summary, Dask together with MDAnalysis makes it straightforward to implement parallel analysis of MD trajectories within a map-reduce scheme.
We show that obtaining good parallel performance depends on multiple factors such as storage system and trajectory file format and provide guidelines for how to optimize trajectory analysis throughput within the constraints of a heterogeneous research computing environment.
Nevertheless, implementing robust parallel trajectory analysis that scales over many nodes remains a challenge.


Acknowledgments
===============

MK and IP were supported by grant ACI-1443054 from the National Science Foundation.
SJ and OB were supported in part by grant ACI-1443054 from the National Science Foundation.
Computational resources were in part provided by the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1053575 (allocation MCB130177 to OB and allocation TG-MCB090174 to SJ) and by Arizona State University Research Computing.



References
==========
.. We use a bibtex file ``mdanalysis.bib`` and use
.. :cite:`Michaud-Agrawal:2011fu` for citations; do not use manual
.. citations


.. _MDAnalysis: http://mdanalysis.org
.. _`MDAnalysis.lib.distances.distance_array`: http://www.mdanalysis.org/mdanalysis/documentation_pages/lib/distances.html.. 
.. _multiprocessing: https://docs.python.org/2/library/multiprocessing.html
.. _joblib: https://pypi.python.org/pypi/joblib
.. _mpi4py: https://mpi4py.scipy.org/
.. _Dask: http://dask.pydata.org
.. _distributed: https://distributed.readthedocs.io/
.. _10.6084/m9.figshare.5108170: https://doi.org/10.6084/m9.figshare.5108170
.. _plugin: https://github.com/radical-cybertools/midas/blob/master/Dask/schedulerPlugin.py
