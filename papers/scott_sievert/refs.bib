@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{bottou2018optimization,
  author = {Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  title = {Optimization methods for large-scale machine learning},
  journal = {SIAM Review},
  volume = {60},
  number = {},
  pages = {223–223},
  year = {2018},
}

@misc{sievert2021improving,
  author = {Sievert, Scott},
  title = {Improving the convergence of SGD through adaptive batch sizes},
  journal = {arXiv preprint arXiv:1910.08222},
  volume = {},
  number = {},
  pages = {},
  year = {2020},
}

@Article{smith2017,
author = {Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc Vgra},
title = {Don't decay the learning rate, increase the batch size},
journal = {arXiv preprint arXiv:1711.00489},
volume = {},
number = {},
pages = {},
year = {2017},
}

@Article{goyal2017accurate,
  author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  title = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  journal = {arXiv preprint arXiv:1706.02677},
  volume = {},
  number = {},
  pages = {},
  year = {2017},
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={arXiv preprint arXiv:1912.01703},
  year={2019}
}

@inproceedings{zhou2018new,
  title={New insight into hybrid stochastic gradient descent: Beyond with-replacement sampling and convexity},
  author={Zhou, Pan and Yuan, Xiao-Tong and Feng, Jiashi},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={1242--1251},
  year={2018}
}

@article{de2016big,
  author = {De, Soham and Yadav, Abhay and Jacobs, David and Goldstein, Tom},
  title = {{Big Batch SGD}: {Automated} Inference using Adaptive Batch Sizes},
  journal = {arXiv preprint arXiv:1610.05792},
  volume = {},
  number = {},
  pages = {},
  year = {2016},
}

@inproceedings{balles2016coupling,
  title={Coupling Adaptive Batch Sizes with Learning Rates},
  author={Balles, Lukas and Romero, Javier and Hennig, Philipp},
  booktitle={33rd Conference on Uncertainty in Artificial Intelligence (UAI 2017)},
  pages={675--684},
  year={2017},
  organization={Curran Associates, Inc.}
}

@article{byrd2012,
  author = {Byrd, Richard H and Chin, Gillian M and Nocedal, Jorge and Wu, Yuchen},
  title = {Sample size selection in optimization methods for machine learning},
  journal = {Mathematical programming},
  volume = {134},
  number = {1},
  pages = {127–155},
  year = {2012}
}

@article{adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@article{adagrad,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@article{asgd,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}


@Article{zagoruyko2016b,
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  title = {Wide residual networks},
  journal = {arXiv preprint arXiv:1605.07146},
  volume = {},
  number = {},
  pages = {},
  year = {2016},
}

@Article{cifar10,
author = {Krizhevsky, Alex and Hinton, Geoffrey},
title = {Learning multiple layers of features from tiny images},
journal = {},
volume = {},
number = {},
pages = {},
year = {2009},
}

@inproceedings{rmsprop,
  title={A sufficient condition for convergences of adam and rmsprop},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11127--11135},
  year={2019}
}

@inproceedings{yin2018,
  author = {Yin, Dong and Pananjady, Ashwin and Lam, Max and Papailiopoulos, Dimitris and Ramchandran, Kannan and Bartlett, Peter},
  editor = {},
  title = {Gradient diversity: a key ingredient for scalable distributed learning},
  booktitle = {Gradient diversity: a key ingredient for scalable distributed learning},
  volume = {International Conference on Artificial Intelligence and Statistics},
  publisher = {},
  address = {},
  pages = {1998-2007},
  year = {2018},
}

@article{khaled2020unified,
  title={Unified analysis of stochastic gradient methods for composite convex and smooth optimization},
  author={Khaled, Ahmed and Sebbouh, Othmane and Loizou, Nicolas and Gower, Robert M and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2006.11573},
  year={2020}
}

@inproceedings{gazagnadou2019optimal,
  title={Optimal mini-batch and step sizes for SAGA},
  author={Gazagnadou, Nidham and Gower, Robert and Salmon, Joseph},
  booktitle={International conference on machine learning},
  pages={2142--2150},
  year={2019},
  organization={PMLR}
}

@article{perrone2019optimal,
  title={Optimal Mini-Batch Size Selection for Fast Gradient Descent},
  author={Perrone, Michael P and Khan, Haidar and Kim, Changhoan and Kyrillidis, Anastasios and Quinn, Jerry and Salapura, Valentina},
  journal={arXiv preprint arXiv:1911.06459},
  year={2019}
}

@article{zhang2019algorithmic,
  title={Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model},
  author={Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George E and Shallue, Christopher J and Grosse, Roger},
  journal={arXiv preprint arXiv:1907.04164},
  year={2019}
}

@inproceedings{johnson2020adascale,
  title={AdaScale SGD: A User-Friendly Algorithm for Distributed Training},
  author={Johnson, Tyler and Agrawal, Pulkit and Gu, Haijie and Guestrin, Carlos},
  booktitle={International Conference on Machine Learning},
  pages={4911--4920},
  year={2020},
  organization={PMLR}
}

@article{alistarh2016qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={arXiv preprint arXiv:1610.02132},
  year={2016}
}

@article{wang2018atomo,
  title={Atomo: Communication-efficient learning via atomic sparsification},
  author={Wang, Hongyi and Sievert, Scott and Charles, Zachary and Liu, Shengchao and Wright, Stephen and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:1806.04090},
  year={2018}
}

@inproceedings{grubic2018synchronous,
  title={Synchronous multi-gpu deep learning with low-precision communication: An experimental study},
  author={Grubic, Demjan and Tam, Leo K and Alistarh, Dan and Zhang, Ce},
  booktitle={Proceedings of the 21st International Conference on Extending Database Technology},
  pages={145--156},
  year={2018},
  organization={OpenProceedings}
}

@article{sergeev2018horovod,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}

@article{li2020pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}

@article{dekel2012optimal,
  title={Optimal Distributed Online Prediction Using Mini-Batches.},
  author={Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={1},
  year={2012}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}


@inproceedings{abadi2016,
  author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael},
  editor = {},
  title = {Tensorflow: A system for large-scale machine learning},
  booktitle = {Tensorflow: A system for large-scale machine learning},
  volume = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
  publisher = {},
  address = {},
  pages = {265-283},
  year = {2016},
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@article{jia2018,
author = {Jia, Xianyan and Song, Shutao and He, Wei and Wang, Yangzihao and Rong, Haidong and Zhou, Feihu and Xie, Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei},
title = {Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes},
journal = {arXiv preprint arXiv:1807.11205},
volume = {},
number = {},
pages = {},
year = {2018},
}

@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

@article{golmant2018computational,
  title={On the computational inefficiency of large batch sizes for stochastic gradient descent},
  author={Golmant, Noah and Vemuri, Nikita and Yao, Zhewei and Feinberg, Vladimir and Gholami, Amir and Rothauge, Kai and Mahoney, Michael W and Gonzalez, Joseph},
  journal={arXiv preprint arXiv:1811.12941},
  year={2018}
}

@article{li2014scaling,
author = {Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
title = {Scaling Distributed Machine Learning with the Parameter Server.},
journal = {OSDI},
volume = {1},
number = {10.4},
pages = {3},
year = {2014},
}

@inproceedings{qi2017paleo,
  title={Paleo: A Performance Model for Deep Neural Networks.},
  author={Qi, Hang and Sparks, Evan R and Talwalkar, Ameet},
  booktitle={ICLR (Poster)},
  year={2017},
  url={https://talwalkarlab.github.io/paleo/},
}


@book{nesterov2013a,
  author = {Nesterov, Yurii},
  title = {Introductory lectures on convex optimization: A basic course},
  volume = {87},
  pages = {},
  editor = {},
  publisher = {Springer Science \& Business Media},
  address = {},
  year = {2013},
}

@Article{recht2011hogwild,
author = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
title = {Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
journal = {Advances in Neural Information Processing Systems},
volume = {},
number = {},
pages = {693–693},
year = {2011},
}


@Article{zhang2016hogwild++,
author = {Zhang, Huan and Hsieh, Cho-Jui and Akella, Venkatesh},
title = {HogWild++: A New Mechanism for Decentralized Asynchronous Stochastic Gradient Descent},
journal = {},
volume = {},
number = {},
pages = {},
year = {2016},
}

@Article{dean2012large,
author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and others},
title = {Large scale distributed deep networks},
journal = {Advances in neural information processing systems},
volume = {},
number = {},
pages = {1223–1223},
year = {2012},
}





